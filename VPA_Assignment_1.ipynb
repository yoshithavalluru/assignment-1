{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmT0VMf23vThEJdz9kBx60",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshithavalluru/assignment-1/blob/main/VPA_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da94t5uwCUDA",
        "outputId": "dfa87337-1058-470c-e8dd-7e5bccff69cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.27.1 -q\n",
        "!pip install gymnasium[box2d] -q\n",
        "!pip install moviepy -q\n",
        "!pip install -U kora -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch.cuda\n",
        "import torch\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "\n",
        "def apply_reward_to_go(raw_reward):\n",
        "    # Reverse the list of rewards\n",
        "    raw_reward = raw_reward[::-1]\n",
        "\n",
        "    # Calculate the running sum of rewards\n",
        "    running_sum = 0\n",
        "    rtg_rewards = []\n",
        "    for r in raw_reward:\n",
        "        running_sum = r + running_sum\n",
        "        rtg_rewards.append(running_sum)\n",
        "\n",
        "    # Reverse the rtg_rewards list again to match the order of the original list\n",
        "    rtg_rewards = rtg_rewards[::-1]\n",
        "\n",
        "    # Normalize the rewards\n",
        "    rtg_rewards = np.array(rtg_rewards)\n",
        "    rtg_rewards = (rtg_rewards - np.mean(rtg_rewards)) / (np.std(rtg_rewards) + np.finfo(np.float32).eps)\n",
        "\n",
        "    # Convert the rtg_rewards to a PyTorch tensor and return\n",
        "    return torch.tensor(rtg_rewards, dtype=torch.float32, device=get_device())\n",
        "\n",
        "\n",
        "def apply_discount(raw_reward, gamma=0.99):\n",
        "    # Reverse the list of rewards\n",
        "    raw_reward = raw_reward[::-1]\n",
        "\n",
        "    # Calculate the discounted reward\n",
        "    running_sum = 0\n",
        "    discounted_rtg_reward = []\n",
        "    for r in raw_reward:\n",
        "        running_sum = r + gamma * running_sum\n",
        "        discounted_rtg_reward.append(running_sum)\n",
        "\n",
        "    # Reverse the discounted_rtg_reward list again to match the order of the original list\n",
        "    discounted_rtg_reward = discounted_rtg_reward[::-1]\n",
        "\n",
        "    # Normalize the rewards\n",
        "    discounted_rtg_reward = np.array(discounted_rtg_reward)\n",
        "    discounted_rtg_reward = (discounted_rtg_reward - np.mean(discounted_rtg_reward)) / (np.std(discounted_rtg_reward) + np.finfo(np.float32).eps)\n",
        "\n",
        "    # Convert the discounted_rtg_reward to a PyTorch tensor and return\n",
        "    return torch.tensor(discounted_rtg_reward, dtype=torch.float32, device=get_device())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Util function to apply reward-return (cumulative reward) on a list of instant-reward (from eq 6)\n",
        "def apply_return(raw_reward):\n",
        "    # Compute r_reward (as a list) from raw_reward\n",
        "    r_reward = [np.sum(raw_reward) for _ in raw_reward]\n",
        "    return torch.tensor(r_reward, dtype=torch.float32, device=get_device())"
      ],
      "metadata": {
        "id": "UabyT0qCCcS5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "class PGTrainer:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        self.env = gym.make(self.params['env_name'])\n",
        "        self.agent = Agent(env=self.env, params=self.params)\n",
        "        self.actor_policy = PGPolicy(input_size=self.env.observation_space.shape[0], output_size=self.env.action_space.n, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.optimizer = Adam(params=self.actor_policy.parameters(), lr=self.params['lr'])\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        list_ro_reward = list()\n",
        "\n",
        "        for ro_idx in range(self.params['n_rollout']):\n",
        "            trajectory = self.agent.collect_trajectory(policy=self.actor_policy)\n",
        "            loss = self.estimate_loss_function(trajectory)\n",
        "            self.update_policy(loss)\n",
        "            # Calculate the average reward for each trajectory\n",
        "      \n",
        "            total_ro_reward = 0\n",
        "            ntr = self.params['n_trajectory_per_rollout']\n",
        "            for tr_idx in range(ntr):\n",
        "                start_idx = tr_idx * self.params['n_rollout']\n",
        "                end_idx = start_idx + self.params['n_rollout']\n",
        "                total_ro_reward += sum([sum(r) for r in trajectory['reward'][start_idx:end_idx]])\n",
        "\n",
        "            avg_ro_reward = total_ro_reward / ntr\n",
        "            print(f'End of rollout {ro_idx}: Average trajectory reward is {avg_ro_reward: 0.2f}')\n",
        "            # Append average rollout reward into a list\n",
        "            list_ro_reward.append(avg_ro_reward)\n",
        "        # Save avg-rewards as pickle files\n",
        "        pkl_file_name = self.params['exp_name'] + '.pkl'\n",
        "        with open(pkl_file_name, 'wb') as f:\n",
        "            pickle.dump(list_ro_reward, f)\n",
        "        # Save a video of the trained agent playing\n",
        "        self.generate_video()\n",
        "        # Close environment\n",
        "        self.env.close()\n",
        "\n",
        "    \n",
        "    def estimate_loss_function(self, trajectory):\n",
        "        loss = []\n",
        "        for traj_idx in range(self.params['n_trajectory_per_rollout']):\n",
        "        # Get the rewards for the current trajectory\n",
        "            reward = torch.tensor(trajectory['reward'][traj_idx], dtype=torch.float32)\n",
        "        \n",
        "        # Get the log-probs for the current trajectory\n",
        "            log_prob = trajectory['log_prob'][traj_idx]\n",
        "        \n",
        "        # Compute the loss based on the flags\n",
        "            if self.params['reward_to_go'] and self.params['reward_discount']:\n",
        "                rtg_reward = apply_reward_to_go(reward.tolist)\n",
        "                discounted_reward = apply_discount(reward.tolist())\n",
        "                rtg_discounted_reward = apply_discount(rtg_reward)\n",
        "                loss.append(-1 * (rtg_discounted_reward * log_prob).mean())\n",
        "            elif self.params['reward_to_go']:\n",
        "                rtg_reward = apply_reward_to_go(reward.tolist())\n",
        "                loss.append(-1 * (rtg_reward * log_prob).mean())\n",
        "            elif self.params['reward_discount']:\n",
        "                discounted_reward = apply_discount(reward.tolist())\n",
        "                loss.append(-1 * (discounted_reward * log_prob).mean())\n",
        "            else:\n",
        "                r_reward = apply_return(reward.tolist())\n",
        "                loss.append(-1 * (r_reward * log_prob).mean())  \n",
        "        loss = torch.stack(loss).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_policy(self, loss):\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def generate_video(self, max_frame=1000):\n",
        "        env_name = self.params['env_name']\n",
        "        exp_name = self.params['exp_name']\n",
        "        self.env = gym.make(env_name, render_mode='rgb_array_list')\n",
        "        obs, _ = self.env.reset()\n",
        "        for _ in range(max_frame):\n",
        "            action_idx, log_prob = self.actor_policy(torch.tensor(obs, dtype=torch.float32, device=get_device()))\n",
        "            obs, reward, terminated, truncated, info = self.env.step(self.agent.action_space[action_idx.item()])\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        video_name = f\"{exp_name}_{env_name[:-3]}\"\n",
        "        save_video(frames=self.env.render(), video_folder=video_name, fps=self.env.metadata['render_fps'], step_starting_index=0, episode_index=0)\n",
        "        self.env.close()\n",
        "\n",
        "class PGPolicy(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(PGPolicy, self).__init__()\n",
        "        # Define the policy net\n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # Forward pass of policy net\n",
        "        policy_dist = Categorical(self.policy_net(obs))\n",
        "        action_index = policy_dist.sample()\n",
        "        log_prob = policy_dist.log_prob(action_index)\n",
        "        return action_index, log_prob\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, params=None):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "        self.action_space = [action for action in range(self.env.action_space.n)]\n",
        "\n",
        "    def collect_trajectory(self, policy):\n",
        "        obs, _ = self.env.reset(seed=self.params['rng_seed'])\n",
        "        rollout_buffer = list()\n",
        "        for _ in range(self.params['n_trajectory_per_rollout']):\n",
        "            trajectory_buffer = {'log_prob': list(), 'reward': list()}\n",
        "            while True:\n",
        "                # Get action from the policy (forward pass of policy net)\n",
        "                action_idx, log_prob = policy.forward(torch.from_numpy(obs).float())\n",
        "                obs, reward, terminated, truncated, info = self.env.step(self.action_space[action_idx.item()])\n",
        "                # Save log-prob and reward into the buffer\n",
        "                trajectory_buffer['log_prob'].append(log_prob)\n",
        "                trajectory_buffer['reward'].append(reward)\n",
        "                # Check for termination criteria\n",
        "                if terminated or truncated:\n",
        "                    obs, _ = self.env.reset(seed=self.params['rng_seed'])\n",
        "                    rollout_buffer.append(trajectory_buffer)\n",
        "                    break\n",
        "        rollout_buffer = self.serialize_trajectory(rollout_buffer)\n",
        "        return rollout_buffer\n",
        "\n",
        "    # Converts a list-of-dictionary into dictionary-of-list\n",
        "    @staticmethod\n",
        "    def serialize_trajectory(rollout_buffer):\n",
        "        serialized_buffer = {'log_prob': list(), 'reward': list()}\n",
        "        for trajectory_buffer in rollout_buffer:\n",
        "            serialized_buffer['log_prob'].append(torch.stack(trajectory_buffer['log_prob']))\n",
        "            serialized_buffer['reward'].append(trajectory_buffer['reward'])\n",
        "        return serialized_buffer"
      ],
      "metadata": {
        "id": "7B9WEVb_CmDw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd ADV AI\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mcJvkLDCyYf",
        "outputId": "efbe88f1-aa99-4afc-dc37-0153455de52f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'ADV AI'\n",
            "/content/gdrive/My Drive/ADV AI\n",
            "CartPole_v1_t0_CartPole  LunarLander-v2_t0_LunarLander\n",
            "CartPole_v1_t0.pkl\t LunarLander-v2_t0.pkl\n",
            "CartPole_v1_t1_CartPole  LunarLander-v2_t1_LunarLander\n",
            "CartPole_v1_t1.pkl\t LunarLander-v2_t1.pkl\n",
            "CartPole_v1_t2_CartPole  LunarLander-v2_t2_LunarLander\n",
            "CartPole_v1_t2.pkl\t LunarLander-v2_t2.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define four sets of parameters\n",
        "params_list = [\n",
        "    {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'rng_seed': 6369,\n",
        "        'reward_to_go': False,\n",
        "        'reward_discount': False,\n",
        "        'n_rollout': 100,\n",
        "        'n_trajectory_per_rollout': 10,\n",
        "        'hidden_dim': 64,\n",
        "        'lr': 3e-3,\n",
        "        'exp_name': 'CartPole_v1_t0'\n",
        "    },\n",
        "    {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'rng_seed': 6369,\n",
        "        'reward_to_go': True,\n",
        "        'reward_discount': False,\n",
        "        'n_rollout': 100,\n",
        "        'n_trajectory_per_rollout': 10,\n",
        "        'hidden_dim': 64,\n",
        "        'lr': 3e-3,\n",
        "        'exp_name': 'CartPole_v1_t1'\n",
        "    },\n",
        "    {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'rng_seed': 6369,\n",
        "        'reward_to_go': False,\n",
        "        'reward_discount': True,\n",
        "        'n_rollout': 100,\n",
        "        'n_trajectory_per_rollout': 10,\n",
        "        'hidden_dim': 64,\n",
        "        'lr': 3e-3,\n",
        "        'exp_name': 'CartPole_v1_t2'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Train agent with each set of parameters\n",
        "for params in params_list:\n",
        "  # Seed RNGs\n",
        "    seed_everything(params['rng_seed'])\n",
        "\n",
        "    # Train agent\n",
        "    trainer = PGTrainer(params)\n",
        "    trainer.run_training_loop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h05LnQHcC1HM",
        "outputId": "e0102330-c2c7-4c59-a7bb-ab58ad36470f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End of rollout 0: Average trajectory reward is  21.40\n",
            "End of rollout 1: Average trajectory reward is  21.10\n",
            "End of rollout 2: Average trajectory reward is  24.90\n",
            "End of rollout 3: Average trajectory reward is  18.40\n",
            "End of rollout 4: Average trajectory reward is  18.10\n",
            "End of rollout 5: Average trajectory reward is  20.60\n",
            "End of rollout 6: Average trajectory reward is  22.80\n",
            "End of rollout 7: Average trajectory reward is  21.50\n",
            "End of rollout 8: Average trajectory reward is  20.00\n",
            "End of rollout 9: Average trajectory reward is  15.60\n",
            "End of rollout 10: Average trajectory reward is  17.10\n",
            "End of rollout 11: Average trajectory reward is  17.30\n",
            "End of rollout 12: Average trajectory reward is  18.70\n",
            "End of rollout 13: Average trajectory reward is  21.00\n",
            "End of rollout 14: Average trajectory reward is  21.00\n",
            "End of rollout 15: Average trajectory reward is  17.70\n",
            "End of rollout 16: Average trajectory reward is  17.70\n",
            "End of rollout 17: Average trajectory reward is  29.00\n",
            "End of rollout 18: Average trajectory reward is  14.10\n",
            "End of rollout 19: Average trajectory reward is  16.40\n",
            "End of rollout 20: Average trajectory reward is  20.80\n",
            "End of rollout 21: Average trajectory reward is  15.80\n",
            "End of rollout 22: Average trajectory reward is  18.60\n",
            "End of rollout 23: Average trajectory reward is  22.80\n",
            "End of rollout 24: Average trajectory reward is  14.80\n",
            "End of rollout 25: Average trajectory reward is  20.30\n",
            "End of rollout 26: Average trajectory reward is  19.70\n",
            "End of rollout 27: Average trajectory reward is  16.90\n",
            "End of rollout 28: Average trajectory reward is  16.60\n",
            "End of rollout 29: Average trajectory reward is  16.50\n",
            "End of rollout 30: Average trajectory reward is  19.90\n",
            "End of rollout 31: Average trajectory reward is  15.70\n",
            "End of rollout 32: Average trajectory reward is  13.60\n",
            "End of rollout 33: Average trajectory reward is  16.60\n",
            "End of rollout 34: Average trajectory reward is  17.80\n",
            "End of rollout 35: Average trajectory reward is  19.10\n",
            "End of rollout 36: Average trajectory reward is  21.30\n",
            "End of rollout 37: Average trajectory reward is  18.60\n",
            "End of rollout 38: Average trajectory reward is  19.30\n",
            "End of rollout 39: Average trajectory reward is  13.70\n",
            "End of rollout 40: Average trajectory reward is  17.50\n",
            "End of rollout 41: Average trajectory reward is  17.80\n",
            "End of rollout 42: Average trajectory reward is  18.90\n",
            "End of rollout 43: Average trajectory reward is  17.40\n",
            "End of rollout 44: Average trajectory reward is  21.60\n",
            "End of rollout 45: Average trajectory reward is  14.40\n",
            "End of rollout 46: Average trajectory reward is  16.60\n",
            "End of rollout 47: Average trajectory reward is  17.30\n",
            "End of rollout 48: Average trajectory reward is  17.60\n",
            "End of rollout 49: Average trajectory reward is  15.80\n",
            "End of rollout 50: Average trajectory reward is  15.60\n",
            "End of rollout 51: Average trajectory reward is  16.00\n",
            "End of rollout 52: Average trajectory reward is  18.30\n",
            "End of rollout 53: Average trajectory reward is  15.70\n",
            "End of rollout 54: Average trajectory reward is  16.90\n",
            "End of rollout 55: Average trajectory reward is  20.40\n",
            "End of rollout 56: Average trajectory reward is  18.70\n",
            "End of rollout 57: Average trajectory reward is  17.50\n",
            "End of rollout 58: Average trajectory reward is  18.10\n",
            "End of rollout 59: Average trajectory reward is  17.30\n",
            "End of rollout 60: Average trajectory reward is  16.40\n",
            "End of rollout 61: Average trajectory reward is  15.70\n",
            "End of rollout 62: Average trajectory reward is  14.90\n",
            "End of rollout 63: Average trajectory reward is  18.60\n",
            "End of rollout 64: Average trajectory reward is  15.60\n",
            "End of rollout 65: Average trajectory reward is  17.60\n",
            "End of rollout 66: Average trajectory reward is  18.90\n",
            "End of rollout 67: Average trajectory reward is  15.70\n",
            "End of rollout 68: Average trajectory reward is  17.90\n",
            "End of rollout 69: Average trajectory reward is  16.00\n",
            "End of rollout 70: Average trajectory reward is  16.90\n",
            "End of rollout 71: Average trajectory reward is  21.00\n",
            "End of rollout 72: Average trajectory reward is  14.00\n",
            "End of rollout 73: Average trajectory reward is  14.70\n",
            "End of rollout 74: Average trajectory reward is  16.90\n",
            "End of rollout 75: Average trajectory reward is  17.50\n",
            "End of rollout 76: Average trajectory reward is  16.10\n",
            "End of rollout 77: Average trajectory reward is  16.80\n",
            "End of rollout 78: Average trajectory reward is  17.30\n",
            "End of rollout 79: Average trajectory reward is  29.10\n",
            "End of rollout 80: Average trajectory reward is  15.20\n",
            "End of rollout 81: Average trajectory reward is  17.20\n",
            "End of rollout 82: Average trajectory reward is  16.70\n",
            "End of rollout 83: Average trajectory reward is  20.10\n",
            "End of rollout 84: Average trajectory reward is  15.00\n",
            "End of rollout 85: Average trajectory reward is  17.30\n",
            "End of rollout 86: Average trajectory reward is  17.90\n",
            "End of rollout 87: Average trajectory reward is  17.00\n",
            "End of rollout 88: Average trajectory reward is  20.80\n",
            "End of rollout 89: Average trajectory reward is  14.20\n",
            "End of rollout 90: Average trajectory reward is  20.50\n",
            "End of rollout 91: Average trajectory reward is  16.20\n",
            "End of rollout 92: Average trajectory reward is  13.90\n",
            "End of rollout 93: Average trajectory reward is  18.10\n",
            "End of rollout 94: Average trajectory reward is  17.70\n",
            "End of rollout 95: Average trajectory reward is  23.40\n",
            "End of rollout 96: Average trajectory reward is  12.40\n",
            "End of rollout 97: Average trajectory reward is  20.60\n",
            "End of rollout 98: Average trajectory reward is  14.90\n",
            "End of rollout 99: Average trajectory reward is  15.80\n",
            "Moviepy - Building video /content/gdrive/MyDrive/ADV AI/CartPole_v1_t0_CartPole/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/gdrive/MyDrive/ADV AI/CartPole_v1_t0_CartPole/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gdrive/MyDrive/ADV AI/CartPole_v1_t0_CartPole/rl-video-episode-0.mp4\n",
            "End of rollout 0: Average trajectory reward is  21.40\n",
            "End of rollout 1: Average trajectory reward is  21.40\n",
            "End of rollout 2: Average trajectory reward is  23.50\n",
            "End of rollout 3: Average trajectory reward is  18.50\n",
            "End of rollout 4: Average trajectory reward is  17.90\n",
            "End of rollout 5: Average trajectory reward is  18.00\n",
            "End of rollout 6: Average trajectory reward is  20.40\n",
            "End of rollout 7: Average trajectory reward is  15.40\n",
            "End of rollout 8: Average trajectory reward is  16.50\n",
            "End of rollout 9: Average trajectory reward is  20.30\n",
            "End of rollout 10: Average trajectory reward is  19.60\n",
            "End of rollout 11: Average trajectory reward is  23.20\n",
            "End of rollout 12: Average trajectory reward is  16.90\n",
            "End of rollout 13: Average trajectory reward is  20.10\n",
            "End of rollout 14: Average trajectory reward is  25.20\n",
            "End of rollout 15: Average trajectory reward is  19.30\n",
            "End of rollout 16: Average trajectory reward is  20.70\n",
            "End of rollout 17: Average trajectory reward is  31.30\n",
            "End of rollout 18: Average trajectory reward is  17.40\n",
            "End of rollout 19: Average trajectory reward is  22.20\n",
            "End of rollout 20: Average trajectory reward is  28.00\n",
            "End of rollout 21: Average trajectory reward is  27.70\n",
            "End of rollout 22: Average trajectory reward is  23.70\n",
            "End of rollout 23: Average trajectory reward is  31.10\n",
            "End of rollout 24: Average trajectory reward is  31.40\n",
            "End of rollout 25: Average trajectory reward is  26.30\n",
            "End of rollout 26: Average trajectory reward is  31.70\n",
            "End of rollout 27: Average trajectory reward is  33.70\n",
            "End of rollout 28: Average trajectory reward is  30.20\n",
            "End of rollout 29: Average trajectory reward is  41.60\n",
            "End of rollout 30: Average trajectory reward is  41.10\n",
            "End of rollout 31: Average trajectory reward is  54.60\n",
            "End of rollout 32: Average trajectory reward is  67.30\n",
            "End of rollout 33: Average trajectory reward is  58.60\n",
            "End of rollout 34: Average trajectory reward is  54.60\n",
            "End of rollout 35: Average trajectory reward is  65.70\n",
            "End of rollout 36: Average trajectory reward is  48.60\n",
            "End of rollout 37: Average trajectory reward is  49.60\n",
            "End of rollout 38: Average trajectory reward is  47.70\n",
            "End of rollout 39: Average trajectory reward is  54.00\n",
            "End of rollout 40: Average trajectory reward is  51.40\n",
            "End of rollout 41: Average trajectory reward is  68.90\n",
            "End of rollout 42: Average trajectory reward is  65.80\n",
            "End of rollout 43: Average trajectory reward is  87.90\n",
            "End of rollout 44: Average trajectory reward is  82.60\n",
            "End of rollout 45: Average trajectory reward is  76.60\n",
            "End of rollout 46: Average trajectory reward is  69.90\n",
            "End of rollout 47: Average trajectory reward is  98.80\n",
            "End of rollout 48: Average trajectory reward is  110.50\n",
            "End of rollout 49: Average trajectory reward is  110.40\n",
            "End of rollout 50: Average trajectory reward is  161.00\n",
            "End of rollout 51: Average trajectory reward is  110.90\n",
            "End of rollout 52: Average trajectory reward is  174.50\n",
            "End of rollout 53: Average trajectory reward is  201.60\n",
            "End of rollout 54: Average trajectory reward is  218.80\n",
            "End of rollout 55: Average trajectory reward is  276.00\n",
            "End of rollout 56: Average trajectory reward is  319.90\n",
            "End of rollout 57: Average trajectory reward is  269.20\n",
            "End of rollout 58: Average trajectory reward is  286.90\n",
            "End of rollout 59: Average trajectory reward is  333.50\n",
            "End of rollout 60: Average trajectory reward is  333.40\n",
            "End of rollout 61: Average trajectory reward is  354.70\n",
            "End of rollout 62: Average trajectory reward is  288.80\n",
            "End of rollout 63: Average trajectory reward is  369.30\n",
            "End of rollout 64: Average trajectory reward is  237.60\n",
            "End of rollout 65: Average trajectory reward is  196.70\n",
            "End of rollout 66: Average trajectory reward is  297.60\n",
            "End of rollout 67: Average trajectory reward is  322.10\n",
            "End of rollout 68: Average trajectory reward is  284.20\n",
            "End of rollout 69: Average trajectory reward is  341.40\n",
            "End of rollout 70: Average trajectory reward is  326.70\n",
            "End of rollout 71: Average trajectory reward is  339.00\n",
            "End of rollout 72: Average trajectory reward is  486.00\n",
            "End of rollout 73: Average trajectory reward is  366.10\n",
            "End of rollout 74: Average trajectory reward is  402.10\n",
            "End of rollout 75: Average trajectory reward is  425.90\n",
            "End of rollout 76: Average trajectory reward is  447.60\n",
            "End of rollout 77: Average trajectory reward is  391.10\n",
            "End of rollout 78: Average trajectory reward is  344.30\n",
            "End of rollout 79: Average trajectory reward is  451.00\n",
            "End of rollout 80: Average trajectory reward is  430.10\n",
            "End of rollout 81: Average trajectory reward is  430.10\n",
            "End of rollout 82: Average trajectory reward is  443.20\n",
            "End of rollout 83: Average trajectory reward is  460.60\n",
            "End of rollout 84: Average trajectory reward is  426.60\n",
            "End of rollout 85: Average trajectory reward is  374.80\n",
            "End of rollout 86: Average trajectory reward is  337.30\n",
            "End of rollout 87: Average trajectory reward is  302.20\n",
            "End of rollout 88: Average trajectory reward is  326.60\n",
            "End of rollout 89: Average trajectory reward is  279.40\n",
            "End of rollout 90: Average trajectory reward is  320.70\n",
            "End of rollout 91: Average trajectory reward is  270.80\n",
            "End of rollout 92: Average trajectory reward is  285.20\n",
            "End of rollout 93: Average trajectory reward is  343.50\n",
            "End of rollout 94: Average trajectory reward is  301.60\n",
            "End of rollout 95: Average trajectory reward is  311.30\n",
            "End of rollout 96: Average trajectory reward is  243.10\n",
            "End of rollout 97: Average trajectory reward is  298.00\n",
            "End of rollout 98: Average trajectory reward is  312.80\n",
            "End of rollout 99: Average trajectory reward is  247.80\n",
            "Moviepy - Building video /content/gdrive/MyDrive/ADV AI/CartPole_v1_t1_CartPole/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/gdrive/MyDrive/ADV AI/CartPole_v1_t1_CartPole/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gdrive/MyDrive/ADV AI/CartPole_v1_t1_CartPole/rl-video-episode-0.mp4\n",
            "End of rollout 0: Average trajectory reward is  21.40\n",
            "End of rollout 1: Average trajectory reward is  21.40\n",
            "End of rollout 2: Average trajectory reward is  23.50\n",
            "End of rollout 3: Average trajectory reward is  18.00\n",
            "End of rollout 4: Average trajectory reward is  19.30\n",
            "End of rollout 5: Average trajectory reward is  17.40\n",
            "End of rollout 6: Average trajectory reward is  23.20\n",
            "End of rollout 7: Average trajectory reward is  17.50\n",
            "End of rollout 8: Average trajectory reward is  15.90\n",
            "End of rollout 9: Average trajectory reward is  19.90\n",
            "End of rollout 10: Average trajectory reward is  29.80\n",
            "End of rollout 11: Average trajectory reward is  18.70\n",
            "End of rollout 12: Average trajectory reward is  16.50\n",
            "End of rollout 13: Average trajectory reward is  26.00\n",
            "End of rollout 14: Average trajectory reward is  19.00\n",
            "End of rollout 15: Average trajectory reward is  20.10\n",
            "End of rollout 16: Average trajectory reward is  29.20\n",
            "End of rollout 17: Average trajectory reward is  21.90\n",
            "End of rollout 18: Average trajectory reward is  25.90\n",
            "End of rollout 19: Average trajectory reward is  20.60\n",
            "End of rollout 20: Average trajectory reward is  33.00\n",
            "End of rollout 21: Average trajectory reward is  24.50\n",
            "End of rollout 22: Average trajectory reward is  26.40\n",
            "End of rollout 23: Average trajectory reward is  30.90\n",
            "End of rollout 24: Average trajectory reward is  28.40\n",
            "End of rollout 25: Average trajectory reward is  40.80\n",
            "End of rollout 26: Average trajectory reward is  26.10\n",
            "End of rollout 27: Average trajectory reward is  33.00\n",
            "End of rollout 28: Average trajectory reward is  35.40\n",
            "End of rollout 29: Average trajectory reward is  37.20\n",
            "End of rollout 30: Average trajectory reward is  55.00\n",
            "End of rollout 31: Average trajectory reward is  44.10\n",
            "End of rollout 32: Average trajectory reward is  60.00\n",
            "End of rollout 33: Average trajectory reward is  60.00\n",
            "End of rollout 34: Average trajectory reward is  60.80\n",
            "End of rollout 35: Average trajectory reward is  70.70\n",
            "End of rollout 36: Average trajectory reward is  65.80\n",
            "End of rollout 37: Average trajectory reward is  51.60\n",
            "End of rollout 38: Average trajectory reward is  66.50\n",
            "End of rollout 39: Average trajectory reward is  70.20\n",
            "End of rollout 40: Average trajectory reward is  61.90\n",
            "End of rollout 41: Average trajectory reward is  55.50\n",
            "End of rollout 42: Average trajectory reward is  58.50\n",
            "End of rollout 43: Average trajectory reward is  69.20\n",
            "End of rollout 44: Average trajectory reward is  94.30\n",
            "End of rollout 45: Average trajectory reward is  113.10\n",
            "End of rollout 46: Average trajectory reward is  110.80\n",
            "End of rollout 47: Average trajectory reward is  91.60\n",
            "End of rollout 48: Average trajectory reward is  116.40\n",
            "End of rollout 49: Average trajectory reward is  127.70\n",
            "End of rollout 50: Average trajectory reward is  106.90\n",
            "End of rollout 51: Average trajectory reward is  133.90\n",
            "End of rollout 52: Average trajectory reward is  153.40\n",
            "End of rollout 53: Average trajectory reward is  168.80\n",
            "End of rollout 54: Average trajectory reward is  229.80\n",
            "End of rollout 55: Average trajectory reward is  268.70\n",
            "End of rollout 56: Average trajectory reward is  314.20\n",
            "End of rollout 57: Average trajectory reward is  319.90\n",
            "End of rollout 58: Average trajectory reward is  424.20\n",
            "End of rollout 59: Average trajectory reward is  294.40\n",
            "End of rollout 60: Average trajectory reward is  325.70\n",
            "End of rollout 61: Average trajectory reward is  318.40\n",
            "End of rollout 62: Average trajectory reward is  341.30\n",
            "End of rollout 63: Average trajectory reward is  296.70\n",
            "End of rollout 64: Average trajectory reward is  295.70\n",
            "End of rollout 65: Average trajectory reward is  320.40\n",
            "End of rollout 66: Average trajectory reward is  294.50\n",
            "End of rollout 67: Average trajectory reward is  226.00\n",
            "End of rollout 68: Average trajectory reward is  187.50\n",
            "End of rollout 69: Average trajectory reward is  195.20\n",
            "End of rollout 70: Average trajectory reward is  166.60\n",
            "End of rollout 71: Average trajectory reward is  182.70\n",
            "End of rollout 72: Average trajectory reward is  121.20\n",
            "End of rollout 73: Average trajectory reward is  147.10\n",
            "End of rollout 74: Average trajectory reward is  132.60\n",
            "End of rollout 75: Average trajectory reward is  140.90\n",
            "End of rollout 76: Average trajectory reward is  113.10\n",
            "End of rollout 77: Average trajectory reward is  95.60\n",
            "End of rollout 78: Average trajectory reward is  124.80\n",
            "End of rollout 79: Average trajectory reward is  121.30\n",
            "End of rollout 80: Average trajectory reward is  134.60\n",
            "End of rollout 81: Average trajectory reward is  168.40\n",
            "End of rollout 82: Average trajectory reward is  154.40\n",
            "End of rollout 83: Average trajectory reward is  205.90\n",
            "End of rollout 84: Average trajectory reward is  228.10\n",
            "End of rollout 85: Average trajectory reward is  308.30\n",
            "End of rollout 86: Average trajectory reward is  346.30\n",
            "End of rollout 87: Average trajectory reward is  448.00\n",
            "End of rollout 88: Average trajectory reward is  393.60\n",
            "End of rollout 89: Average trajectory reward is  376.00\n",
            "End of rollout 90: Average trajectory reward is  436.50\n",
            "End of rollout 91: Average trajectory reward is  463.20\n",
            "End of rollout 92: Average trajectory reward is  406.50\n",
            "End of rollout 93: Average trajectory reward is  470.30\n",
            "End of rollout 94: Average trajectory reward is  397.50\n",
            "End of rollout 95: Average trajectory reward is  487.70\n",
            "End of rollout 96: Average trajectory reward is  500.00\n",
            "End of rollout 97: Average trajectory reward is  500.00\n",
            "End of rollout 98: Average trajectory reward is  498.30\n",
            "End of rollout 99: Average trajectory reward is  500.00\n",
            "Moviepy - Building video /content/gdrive/MyDrive/ADV AI/CartPole_v1_t2_CartPole/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/gdrive/MyDrive/ADV AI/CartPole_v1_t2_CartPole/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gdrive/MyDrive/ADV AI/CartPole_v1_t2_CartPole/rl-video-episode-0.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load rewards data for each experiment\n",
        "exp_names = ['CartPole_v1_t0', 'CartPole_v1_t1', 'CartPole_v1_t2']\n",
        "colors = ['blue', 'green', 'red']\n",
        "exp_rewards = []\n",
        "for name in exp_names:\n",
        "    with open(name+'.pkl', 'rb') as f:\n",
        "        exp_rewards.append(pickle.load(f))\n",
        "\n",
        "# Plot the data\n",
        "for i in range(len(exp_rewards)):\n",
        "    sns.lineplot(data=exp_rewards[i], color=colors[i], label=exp_names[i])\n",
        "plt.xlabel('rollout', fontsize=25, labelpad=-2)\n",
        "plt.ylabel('reward', fontsize=25)\n",
        "plt.title('Learning curve for CartPole', fontsize=30)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "exp_names = ['LunarLander-v2_t0', 'LunarLander-v2_t1', 'LunarLander-v2_t2']\n",
        "colors = ['blue', 'green', 'red']\n",
        "exp_rewards = []\n",
        "\n",
        "for name in exp_names:\n",
        "    with open(name+'.pkl', 'rb') as f:\n",
        "        exp_rewards.append(pickle.load(f))\n",
        "\n",
        "# Plot the data\n",
        "for i in range(len(exp_rewards)):\n",
        "    sns.lineplot(data=exp_rewards[i], color=colors[i], label=exp_names[i])\n",
        "plt.xlabel('rollout', fontsize=25, labelpad=-2)\n",
        "plt.ylabel('reward', fontsize=25)\n",
        "plt.title('Learning curve for LunarLander', fontsize=30)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9orsaE-GDJzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_names = ['LunarLander-v2_t0', 'LunarLander-v2_t1', 'LunarLander-v2_t2']\n",
        "colors = ['blue', 'green', 'red']\n",
        "exp_rewards = []\n",
        "exp_rewards_mean = []\n",
        "exp_rewards_var = []\n",
        "for name in exp_names:\n",
        "    with open(name+'.pkl', 'rb') as f:\n",
        "        rewards = pickle.load(f)\n",
        "        exp_rewards.append(rewards)\n",
        "        exp_rewards_mean.append(np.mean(rewards, axis=0))\n",
        "        exp_rewards_var.append(np.var(rewards, axis=0))\n",
        "\n",
        "\n",
        "print(exp_rewards_mean)\n",
        "print(exp_rewards_var)\n"
      ],
      "metadata": {
        "id": "-TvwvMu3DWDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_list = [\n",
        "    {\n",
        "        'env_name': 'LunarLander-v2',\n",
        "        'rng_seed': 6369,\n",
        "        'reward_to_go': False,\n",
        "        'reward_discount': True,\n",
        "        'n_rollout': 125,\n",
        "        'n_trajectory_per_rollout': 5,\n",
        "        'hidden_dim': 128,\n",
        "        'lr': 3e-3,\n",
        "        'exp_name': 'LunarLander-v2_t0'\n",
        "    },\n",
        "     {\n",
        "        'env_name': 'LunarLander-v2',\n",
        "        'rng_seed': 6369,\n",
        "        'reward_to_go': False,\n",
        "        'reward_discount': True,\n",
        "        'n_rollout': 125,\n",
        "        'n_trajectory_per_rollout': 20,\n",
        "        'hidden_dim': 128,\n",
        "        'lr': 3e-3,\n",
        "        'exp_name': 'LunarLander-v2_t1'\n",
        "    },\n",
        "      {\n",
        "        'env_name': 'LunarLander-v2',\n",
        "        'rng_seed': 6369,\n",
        "        'reward_to_go': False,\n",
        "        'reward_discount': True,'n_rollout': 125,\n",
        "        'n_trajectory_per_rollout': 60,\n",
        "        'hidden_dim': 128,\n",
        "        'lr': 3e-3,\n",
        "        'exp_name': 'LunarLander-v2_t2'\n",
        "    },\n",
        "]\n",
        "\n",
        "# Train agent with each set of parameters\n",
        "for params in params_list:\n",
        "    # Seed RNGs\n",
        "    seed_everything(params['rng_seed'])\n",
        "\n",
        "    # Train agent\n",
        "    trainer = PGTrainer(params)\n",
        "    trainer.run_training_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPZf_WzADaOT",
        "outputId": "0d32b5e3-149f-4fcd-9ef3-fcf0d6b308ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of rollout 0: Average trajectory reward is -321.95\n",
            "End of rollout 1: Average trajectory reward is -172.70\n",
            "End of rollout 2: Average trajectory reward is -207.05\n",
            "End of rollout 3: Average trajectory reward is -173.51\n",
            "End of rollout 4: Average trajectory reward is -146.09\n",
            "End of rollout 5: Average trajectory reward is -262.84\n",
            "End of rollout 6: Average trajectory reward is -301.92\n",
            "End of rollout 7: Average trajectory reward is -335.43\n",
            "End of rollout 8: Average trajectory reward is -319.98\n",
            "End of rollout 9: Average trajectory reward is -321.26\n",
            "End of rollout 10: Average trajectory reward is -248.44\n",
            "End of rollout 11: Average trajectory reward is -328.46\n",
            "End of rollout 12: Average trajectory reward is -292.44\n",
            "End of rollout 13: Average trajectory reward is -328.40\n",
            "End of rollout 14: Average trajectory reward is -376.50\n",
            "End of rollout 15: Average trajectory reward is -285.01\n",
            "End of rollout 16: Average trajectory reward is -289.34\n",
            "End of rollout 17: Average trajectory reward is -302.57\n",
            "End of rollout 18: Average trajectory reward is -218.93\n",
            "End of rollout 19: Average trajectory reward is -249.76\n",
            "End of rollout 20: Average trajectory reward is -183.82\n",
            "End of rollout 21: Average trajectory reward is -238.47\n",
            "End of rollout 22: Average trajectory reward is -291.45\n",
            "End of rollout 23: Average trajectory reward is -202.82\n",
            "End of rollout 24: Average trajectory reward is -238.60\n",
            "End of rollout 25: Average trajectory reward is -173.08\n",
            "End of rollout 26: Average trajectory reward is -218.40\n",
            "End of rollout 27: Average trajectory reward is -270.70\n",
            "End of rollout 28: Average trajectory reward is -283.44\n",
            "End of rollout 29: Average trajectory reward is -220.92\n",
            "End of rollout 30: Average trajectory reward is -159.80\n",
            "End of rollout 31: Average trajectory reward is -215.39\n",
            "End of rollout 32: Average trajectory reward is -222.55\n",
            "End of rollout 33: Average trajectory reward is -239.77\n",
            "End of rollout 34: Average trajectory reward is -235.64\n",
            "End of rollout 35: Average trajectory reward is -313.68\n",
            "End of rollout 36: Average trajectory reward is -269.38\n",
            "End of rollout 37: Average trajectory reward is -252.69\n",
            "End of rollout 38: Average trajectory reward is -269.69\n",
            "End of rollout 39: Average trajectory reward is -230.82\n",
            "End of rollout 40: Average trajectory reward is -189.72\n",
            "End of rollout 41: Average trajectory reward is -156.36\n",
            "End of rollout 42: Average trajectory reward is -142.83\n",
            "End of rollout 43: Average trajectory reward is -170.86\n",
            "End of rollout 44: Average trajectory reward is -120.24\n",
            "End of rollout 45: Average trajectory reward is -246.85\n",
            "End of rollout 46: Average trajectory reward is -155.02\n",
            "End of rollout 47: Average trajectory reward is -92.31\n",
            "End of rollout 48: Average trajectory reward is -122.56\n",
            "End of rollout 49: Average trajectory reward is -123.05\n",
            "End of rollout 50: Average trajectory reward is -149.99\n",
            "End of rollout 51: Average trajectory reward is -88.49\n",
            "End of rollout 52: Average trajectory reward is -59.61\n",
            "End of rollout 53: Average trajectory reward is -61.71\n",
            "End of rollout 54: Average trajectory reward is -52.44\n",
            "End of rollout 55: Average trajectory reward is -62.12\n",
            "End of rollout 56: Average trajectory reward is -100.05\n",
            "End of rollout 57: Average trajectory reward is -43.11\n",
            "End of rollout 58: Average trajectory reward is -61.88\n",
            "End of rollout 59: Average trajectory reward is -58.86\n",
            "End of rollout 60: Average trajectory reward is -37.18\n",
            "End of rollout 61: Average trajectory reward is -90.47\n",
            "End of rollout 62: Average trajectory reward is -78.95\n",
            "End of rollout 63: Average trajectory reward is -127.91\n",
            "End of rollout 64: Average trajectory reward is -168.33\n",
            "End of rollout 65: Average trajectory reward is -126.10\n",
            "End of rollout 66: Average trajectory reward is -153.09\n",
            "End of rollout 67: Average trajectory reward is -139.93\n",
            "End of rollout 68: Average trajectory reward is -132.41\n",
            "End of rollout 69: Average trajectory reward is -142.25\n",
            "End of rollout 70: Average trajectory reward is -231.65\n",
            "End of rollout 71: Average trajectory reward is -106.69\n",
            "End of rollout 72: Average trajectory reward is -167.92\n",
            "End of rollout 73: Average trajectory reward is -173.10\n",
            "End of rollout 74: Average trajectory reward is -107.20\n",
            "End of rollout 75: Average trajectory reward is -55.88\n",
            "End of rollout 76: Average trajectory reward is -84.04\n",
            "End of rollout 77: Average trajectory reward is -18.27\n",
            "End of rollout 78: Average trajectory reward is -51.55\n",
            "End of rollout 79: Average trajectory reward is -117.78\n",
            "End of rollout 80: Average trajectory reward is -3.99\n",
            "End of rollout 81: Average trajectory reward is -2.24\n",
            "End of rollout 82: Average trajectory reward is  15.61\n",
            "End of rollout 83: Average trajectory reward is -27.88\n",
            "End of rollout 84: Average trajectory reward is -19.26\n",
            "End of rollout 85: Average trajectory reward is  18.53\n",
            "End of rollout 86: Average trajectory reward is -9.66\n",
            "End of rollout 87: Average trajectory reward is -18.54\n",
            "End of rollout 88: Average trajectory reward is -43.16\n",
            "End of rollout 89: Average trajectory reward is -44.32\n",
            "End of rollout 90: Average trajectory reward is -73.72\n",
            "End of rollout 91: Average trajectory reward is -72.19\n",
            "End of rollout 92: Average trajectory reward is -49.79\n",
            "End of rollout 93: Average trajectory reward is -87.07\n",
            "End of rollout 94: Average trajectory reward is -129.75\n",
            "End of rollout 95: Average trajectory reward is  25.84\n",
            "End of rollout 96: Average trajectory reward is -53.49\n",
            "End of rollout 97: Average trajectory reward is  21.95\n",
            "End of rollout 98: Average trajectory reward is  32.46\n",
            "End of rollout 99: Average trajectory reward is  41.68\n",
            "End of rollout 100: Average trajectory reward is  34.54\n",
            "End of rollout 101: Average trajectory reward is  6.36\n",
            "End of rollout 102: Average trajectory reward is  24.95\n",
            "End of rollout 103: Average trajectory reward is  3.82\n",
            "End of rollout 104: Average trajectory reward is  63.59\n",
            "End of rollout 105: Average trajectory reward is  39.85\n",
            "End of rollout 106: Average trajectory reward is -19.19\n",
            "End of rollout 107: Average trajectory reward is -54.06\n",
            "End of rollout 108: Average trajectory reward is  8.25\n",
            "End of rollout 109: Average trajectory reward is  25.13\n",
            "End of rollout 110: Average trajectory reward is  79.94\n",
            "End of rollout 111: Average trajectory reward is  63.75\n",
            "End of rollout 112: Average trajectory reward is  27.29\n",
            "End of rollout 113: Average trajectory reward is  31.83\n",
            "End of rollout 114: Average trajectory reward is  96.58\n",
            "End of rollout 115: Average trajectory reward is  9.32\n",
            "End of rollout 116: Average trajectory reward is  50.30\n",
            "End of rollout 117: Average trajectory reward is  56.09\n",
            "End of rollout 118: Average trajectory reward is  36.33\n",
            "End of rollout 119: Average trajectory reward is  38.55\n",
            "End of rollout 120: Average trajectory reward is  38.59\n",
            "End of rollout 121: Average trajectory reward is  55.09\n",
            "End of rollout 122: Average trajectory reward is  91.32\n",
            "End of rollout 123: Average trajectory reward is  117.71\n",
            "End of rollout 124: Average trajectory reward is  104.66\n",
            "Moviepy - Building video /content/gdrive/MyDrive/ADV AI/LunarLander-v2_t0_LunarLander/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/gdrive/MyDrive/ADV AI/LunarLander-v2_t0_LunarLander/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gdrive/MyDrive/ADV AI/LunarLander-v2_t0_LunarLander/rl-video-episode-0.mp4\n",
            "End of rollout 0: Average trajectory reward is -300.03\n",
            "End of rollout 1: Average trajectory reward is -238.19\n",
            "End of rollout 2: Average trajectory reward is -186.27\n",
            "End of rollout 3: Average trajectory reward is -161.50\n",
            "End of rollout 4: Average trajectory reward is -196.66\n",
            "End of rollout 5: Average trajectory reward is -222.58\n",
            "End of rollout 6: Average trajectory reward is -233.90\n",
            "End of rollout 7: Average trajectory reward is -240.06\n",
            "End of rollout 8: Average trajectory reward is -249.77\n",
            "End of rollout 9: Average trajectory reward is -243.30\n",
            "End of rollout 10: Average trajectory reward is -289.33\n",
            "End of rollout 11: Average trajectory reward is -301.11\n",
            "End of rollout 12: Average trajectory reward is -293.23\n",
            "End of rollout 13: Average trajectory reward is -324.43\n",
            "End of rollout 14: Average trajectory reward is -317.91\n",
            "End of rollout 15: Average trajectory reward is -328.50\n",
            "End of rollout 16: Average trajectory reward is -285.48\n",
            "End of rollout 17: Average trajectory reward is -255.32\n",
            "End of rollout 18: Average trajectory reward is -300.18\n",
            "End of rollout 19: Average trajectory reward is -231.77\n",
            "End of rollout 20: Average trajectory reward is -213.85\n",
            "End of rollout 21: Average trajectory reward is -250.70\n",
            "End of rollout 22: Average trajectory reward is -214.82\n",
            "End of rollout 23: Average trajectory reward is -211.98\n",
            "End of rollout 24: Average trajectory reward is -221.12\n",
            "End of rollout 25: Average trajectory reward is -250.99\n",
            "End of rollout 26: Average trajectory reward is -172.49\n",
            "End of rollout 27: Average trajectory reward is -189.40\n",
            "End of rollout 28: Average trajectory reward is -198.16\n",
            "End of rollout 29: Average trajectory reward is -169.98\n",
            "End of rollout 30: Average trajectory reward is -168.73\n",
            "End of rollout 31: Average trajectory reward is -160.90\n",
            "End of rollout 32: Average trajectory reward is -167.17\n",
            "End of rollout 33: Average trajectory reward is -142.27\n",
            "End of rollout 34: Average trajectory reward is -145.36\n",
            "End of rollout 35: Average trajectory reward is -115.44\n",
            "End of rollout 36: Average trajectory reward is -109.76\n",
            "End of rollout 37: Average trajectory reward is -104.27\n",
            "End of rollout 38: Average trajectory reward is -132.29\n",
            "End of rollout 39: Average trajectory reward is -128.69\n",
            "End of rollout 40: Average trajectory reward is -100.90\n",
            "End of rollout 41: Average trajectory reward is -77.04\n",
            "End of rollout 42: Average trajectory reward is -82.43\n",
            "End of rollout 43: Average trajectory reward is -67.20\n",
            "End of rollout 44: Average trajectory reward is -65.02\n",
            "End of rollout 45: Average trajectory reward is -69.65\n",
            "End of rollout 46: Average trajectory reward is -84.68\n",
            "End of rollout 47: Average trajectory reward is -91.21\n",
            "End of rollout 48: Average trajectory reward is -96.53\n",
            "End of rollout 49: Average trajectory reward is -122.06\n",
            "End of rollout 50: Average trajectory reward is -89.16\n",
            "End of rollout 51: Average trajectory reward is -66.35\n",
            "End of rollout 52: Average trajectory reward is -86.74\n",
            "End of rollout 53: Average trajectory reward is -76.27\n"
          ]
        }
      ]
    }
  ]
}